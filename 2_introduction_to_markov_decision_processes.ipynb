{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-introduction-to-markov-decision-processes.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyP33sUcMT/NRHG8xDk2j6GA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow/blob/18-reinforcement-learning/2_introduction_to_markov_decision_processes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFgQ8q2ao6gf",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Markov Decision Processes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHP2IpiMo7V7",
        "colab_type": "text"
      },
      "source": [
        "In the early 20th century, **the mathematician Andrey Markov studied stochastic processes with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s′ is fixed, and it depends only on the pair (s, s′), not on past states (this is why we say that the system has no memory)**.\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/markov-chain.png?raw=1' width='800'/>\n",
        "\n",
        "Suppose that the process starts in state $s_0$, and there is a 70% chance that it will remain in that state at the next step. Eventually it is bound to leave that state and never come back because no other state points back to $s_0$. If it goes to state $s_1$, it will then most likely go to state $s_2$ (90% probability), then immediately back to state $s_1$ (with 100% probability). It may alternate a number of times between these two states, but eventually it will fall into state $s_3$ and remain there forever (this is a terminal\n",
        "state). Markov chains can have very different dynamics, and they are heavily used in thermodynamics, chemistry, statistics, and much more.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_PLJy4GsaXM",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r1JJGSCsbg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWCrAC2D_H-z",
        "colab_type": "text"
      },
      "source": [
        "## Markov Chains"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVpHkbCUBNTf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "86644af2-ec5f-44ee-ee27-3ec098c29ff0"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# shape=[s, s']\n",
        "transition_probabilities = [\n",
        "    [0.7, 0.2, 0.0, 0.1],  # from s0 to s0, s1, s2, s3\n",
        "    [0.0, 0.0, 0.9, 0.1],  # from s1 to ...\n",
        "    [0.0, 1.0, 0.0, 0.0],  # from s2 to ...\n",
        "    [0.0, 0.0, 0.0, 1.0]   # from s3 to ...                       \n",
        "]\n",
        "\n",
        "n_max_steps = 50\n",
        "\n",
        "def print_sequence():\n",
        "  current_state = 0\n",
        "  print('States:', end=' ')\n",
        "  for step in range(n_max_steps):\n",
        "    print(current_state, end=' ')\n",
        "    if current_state == 3:\n",
        "      break\n",
        "    current_state = np.random.choice(range(4), p=transition_probabilities[current_state])\n",
        "  else:\n",
        "    print('...', end='')\n",
        "  print()\n",
        "\n",
        "for _ in range(10):\n",
        "  print_sequence()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "States: 0 0 3 \n",
            "States: 0 1 2 1 2 1 2 1 2 1 3 \n",
            "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
            "States: 0 3 \n",
            "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n",
            "States: 0 1 3 \n",
            "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 ...\n",
            "States: 0 0 3 \n",
            "States: 0 0 0 1 2 1 2 1 3 \n",
            "States: 0 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 2 1 3 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ-nBGsVFOUT",
        "colab_type": "text"
      },
      "source": [
        "## Markov Decision Process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLy-Elk2FPMy",
        "colab_type": "text"
      },
      "source": [
        "Markov decision processes were first [described in the 1950s by Richard Bellman](https://apps.dtic.mil/dtic/tr/fulltext/u2/606367.pdf).\n",
        "**They resemble Markov chains but with a twist: at each step, an agent can choose one of several possible actions, and the transition probabilities depend on the chosen action.** Moreover, some state transitions return some reward (positive or negative), and the agent’s goal is to find a policy that will maximize reward over time.\n",
        "\n",
        "For example, the MDP has three states (represented by circles) and up to three possible discrete actions at each step (represented by diamonds).\n",
        "\n",
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/markov-decision-process.png?raw=1' width='800'/>\n",
        "\n",
        "If it starts in state $s_0$, the agent can choose between actions $a_0, a_1$, or $a_2$. If it chooses action $a_1$, it just remains in state $s_0$ with certainty, and without any reward. It can thus decide to stay there forever if it wants to. But if it chooses action $a_0$, it has a 70% probability\n",
        "of gaining a reward of +10 and remaining in state $s_0$. \n",
        "\n",
        "It can then try again and again to gain as much reward as possible, but at one point it is going to end up instead in state $s_1$. In state $s_1$ it has only two possible actions: $a_0$ or $a_2$. It can choose to stay put by repeatedly choosing action $a_0$, or it can choose to move on to state $s_2$ and get a negative reward of –50 (ouch). In state $s_2$ it has no other choice than to take action $a_1$, which will most likely lead it back to state $s_0$, gaining a reward of +40 on the way. \n",
        "\n",
        "You get the picture. By looking at this MDP, can you guess which strategy will\n",
        "gain the most reward over time? In state $s_0$ it is clear that action $a_0$ is the best option, and in state $s_2$ the agent has no choice but to take action $a_1$, but in state $s_1$ it is not obvious whether the agent should stay put ($a_0$) or go through the fire ($a_2$).\n",
        "\n",
        "Bellman found a way to estimate the optimal state value of any state $s$, noted $V*(s)$, which is the sum of all discounted future rewards the agent can expect on average after it reaches a state s, assuming it acts optimally.\n",
        "\n",
        "He showed that if the agent acts optimally, then the Bellman Optimality Equation applies.This recursive equation says that if the agent acts optimally, then the optimal value of the current state is equal to the reward it will get on average after taking one optimal action, plus the expected optimal value of all possible next states that this action can lead to.\n",
        "\n",
        "$$V^*(s) = max_a Σ_sT(s,a,s′)[R(s,a,s′) + γ · V*(s′)]$$\n",
        "\n",
        "In this equation:\n",
        "\n",
        "* $T(s, a, s′)$ is the transition probability from state $s$ to state $s′$, given that the agent chose action $a$. For example,$T(s_2, a_1, s_0) = 0.8$.\n",
        "\n",
        "* $R(s, a, s′)$ is the reward that the agent gets when it goes from state $s$ to state $s′$, given that the agent chose action $a$. For example, $R(s2, a1,\n",
        "s0) = +40.$\n",
        "\n",
        "* $γ$ is the discount factor.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXgXt3YME9_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}