{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-generating-shakespearean-text-using-character-rnn.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPdzcTKm9UjQbjvU8LOIs/Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahiakela/hands-on-machine-learning-with-scikit-learn-keras-and-tensorflow/blob/16-natural-nanguage-processing-with-RNNs-and-Attention/1_generating_shakespearean_text_using_character_rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHUB8psfrgPJ",
        "colab_type": "text"
      },
      "source": [
        "# Generating Shakespearean Text Using a Character RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3nxCza6rwcN",
        "colab_type": "text"
      },
      "source": [
        "A common approach for natural language tasks is to use recurrent neural networks.\n",
        "We will therefore continue to explore RNNs, starting with\n",
        "a character RNN, trained to predict the next character in a sentence. This will allow us to generate some original text, and in the process we will see how to build a TensorFlow Dataset on a very long sequence. \n",
        "\n",
        "We will first use a stateless RNN (which learns on random portions of text at each iteration, without any information on the rest of the text), then we will build a stateful RNN (which preserves the hidden state between training iterations and continues reading where it left off, allowing it to learn longer patterns).\n",
        "\n",
        "Let’s start with a simple and fun model that can write like Shakespeare (well, sort of)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLNaLqOquKI4",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wC8SV2kuM11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7659e2e9-3fde-4166-eefe-5db245fc17ff"
      },
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 5)  # Python ≥3.5 is required\n",
        "\n",
        "import sklearn \n",
        "assert sklearn.__version__ >= \"0.20\"  # Scikit-Learn ≥0.20 is required\n",
        "\n",
        "# %tensorflow_version only exists in Colab.\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "  IS_COLAB = True\n",
        "except Exception:\n",
        "  IS_COLAB = False\n",
        "  pass\n",
        "\n",
        "# TensorFlow ≥2.0 is required\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= '2.0'\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU was detected. LSTMs and CNNs can be very slow without a GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3Rly-8iuRJL",
        "colab_type": "text"
      },
      "source": [
        "## Generating Shakespearean Text Using a Character RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztlqRYlpuSAz",
        "colab_type": "text"
      },
      "source": [
        "In a famous 2015 [blog post](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) titled “The Unreasonable Effectiveness of Recurrent Neural Networks,” Andrej Karpathy showed how to train an RNN to predict the next character in a sentence. This Char-RNN can then be used to generate novel text, one character at a time. Here is a small sample of the text generated by a Char-RNN model after it was trained on all of Shakespeare’s work:\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "PANDARUS:\n",
        "\n",
        "Alas, I think he shall be come approached and the day\n",
        "\n",
        "When little srain would be attain’d into being never fed,\n",
        "\n",
        "And who is but a chain and subjects of his death,\n",
        "\n",
        "I should not sleep.\n",
        "\n",
        "---\n",
        "\n",
        "Not exactly a masterpiece, but it is still impressive that the model was able to learn words, grammar, proper punctuation, and more, just by learning to predict the next character in a sentence. \n",
        "\n",
        "Let’s look at how to build a Char-RNN, step by step, starting\n",
        "with the creation of the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuOZsaWcNHXK",
        "colab_type": "text"
      },
      "source": [
        "## Splitting a sequence into batches of shuffled windows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHZEUnE5NM8a",
        "colab_type": "text"
      },
      "source": [
        "For example, let's split the sequence 0 to 14 into windows of length 5, each shifted by 2 (e.g.,`[0, 1, 2, 3, 4]`, `[2, 3, 4, 5, 6]`, etc.), then shuffle them, and split them into inputs (the first 4 steps) and targets (the last 4 steps) (e.g., `[2, 3, 4, 5, 6]` would be split into `[[2, 3, 4, 5], [3, 4, 5, 6]]`), then create batches of 3 such input/target pairs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBMcHqcyNANH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "265db693-379b-42b4-d8f8-1e77aa1f22cb"
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "n_steps = 5\n",
        "dataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\n",
        "dataset = dataset.window(n_steps, shift=2, drop_remainder=True)\n",
        "dataset = dataset.flat_map(lambda window: window.batch(n_steps))\n",
        "dataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:]))\n",
        "dataset = dataset.batch(3).prefetch(1)\n",
        "for index, (X_batch, Y_batch) in enumerate(dataset):\n",
        "    print(\"_\" * 20, \"Batch\", index, \"\\nX_batch\")\n",
        "    print(X_batch.numpy())\n",
        "    print(\"=\" * 5, \"\\nY_batch\")\n",
        "    print(Y_batch.numpy())"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "____________________ Batch 0 \n",
            "X_batch\n",
            "[[6 7 8 9]\n",
            " [2 3 4 5]\n",
            " [4 5 6 7]]\n",
            "===== \n",
            "Y_batch\n",
            "[[ 7  8  9 10]\n",
            " [ 3  4  5  6]\n",
            " [ 5  6  7  8]]\n",
            "____________________ Batch 1 \n",
            "X_batch\n",
            "[[ 0  1  2  3]\n",
            " [ 8  9 10 11]\n",
            " [10 11 12 13]]\n",
            "===== \n",
            "Y_batch\n",
            "[[ 1  2  3  4]\n",
            " [ 9 10 11 12]\n",
            " [11 12 13 14]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odsh5-zAznJC",
        "colab_type": "text"
      },
      "source": [
        "## Creating the Training Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYDdX8lRzsHo",
        "colab_type": "text"
      },
      "source": [
        "First, let’s download all of Shakespeare’s work, using Keras’s handy get_file() function and downloading the data from Andrej Karpathy’s Char-RNN project:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wawoSl90SJO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "outputId": "3405a606-4b9b-4756-f895-c45cd30f5268"
      },
      "source": [
        "shakespeare_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "filepath = keras.utils.get_file('shakespeare.txt', shakespeare_url)\n",
        "\n",
        "with open(filepath) as f:\n",
        "  shakespeare_text = f.read()\n",
        "\n",
        "print(shakespeare_text[:148])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVziKeqC0zLZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "de250e58-0103-4b28-8ef0-eb93ea93023d"
      },
      "source": [
        "\"\".join(sorted(set(shakespeare_text.lower())))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5RuSBA81T6s",
        "colab_type": "text"
      },
      "source": [
        "We must encode every character as an integer. One option is to create a custom preprocessing layer. \n",
        "\n",
        "But in this case, it will be simpler to use Keras’s Tokenizer class. First we need to fit a tokenizer to the text: it will find all the characters used in the text and map each of them to a different character ID, from 1 to the number of distinct characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICTlgY4v1MJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
        "tokenizer.fit_on_texts(shakespeare_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MACJFPKb163k",
        "colab_type": "text"
      },
      "source": [
        "We set char_level=True to get character-level encoding rather than the default word-level encoding. Note that this tokenizer converts the text to lowercase by default (but you can set lower=False if you do not want that). \n",
        "\n",
        "Now the tokenizer can encode a sentence (or a list of sentences) to a list of character IDs and back, and it tells us how many distinct characters there are and the total number of characters in the text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhrGxGIh1s8M",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d0893434-d3b9-45b3-a079-52e316c6d2d6"
      },
      "source": [
        "tokenizer.texts_to_sequences(['First'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[20, 6, 9, 8, 3]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koCa8Zoy2HXJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6eda7d5f-94b5-4851-c006-c36fa5704c70"
      },
      "source": [
        "tokenizer.sequences_to_texts([[20, 6, 8, 3]])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f i s t']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP6tQp0q2PNP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "da2c96f0-2f54-4be9-fb35-149ef305b17c"
      },
      "source": [
        "max_id = len(tokenizer.word_index) # number of distinct characters\n",
        "dataset_size = tokenizer.document_count  # total number of characters\n",
        "print(max_id)\n",
        "print(dataset_size)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39\n",
            "1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OicBIYd520AU",
        "colab_type": "text"
      },
      "source": [
        "Let’s encode the full text so each character is represented by its ID (we subtract 1 to get IDs from 0 to 38, rather than from 1 to 39):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYPjSFAb2bTv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uboycVEm3N4f",
        "colab_type": "text"
      },
      "source": [
        "### How to Split a Sequential Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUyc0rds3RPt",
        "colab_type": "text"
      },
      "source": [
        "It is very important to avoid any overlap between the training set, the validation set, and the test set.\n",
        "\n",
        "When dealing with time series, you would in general split across time,: for example, you might take the years 2000 to 2012 for the training set, the years 2013 to 2015 for the validation set, and the years 2016 to 2018 for the test set. However, in some cases you may be able to split along other dimensions, which will give you a longer time period to train on.\n",
        "\n",
        "So, it is often safer to split across time—but this implicitly assumes that the patterns the RNN can learn in the past (in the training set) will still exist in the future. In other words, we assume that the time series is *stationary* (at least in a wide sense).\n",
        "\n",
        "In short, splitting a time series into a training set, a validation set, and a test set is not a trivial task, and how it’s done will depend strongly on the task at hand.\n",
        "\n",
        "Let’s take the first 90% of the text for the training set\n",
        "(keeping the rest for the validation set and the test set), and create a tf.data.Dataset that will return each character one by one from this set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9nipJwU3M8E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_size = dataset_size * 90 // 100\n",
        "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6HmYaO05AZE",
        "colab_type": "text"
      },
      "source": [
        "### Chopping the Sequential Dataset into Multiple Windows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-rp-6uE5Ban",
        "colab_type": "text"
      },
      "source": [
        "The training set now consists of a single sequence of over a million characters, so we can’t just train the neural network directly on it: the RNN would be equivalent to a deep net with over a million layers, and we would have a single (very long) instance to train it. Instead, we will use the dataset’s window() method to convert this long sequence of characters into many smaller windows of text. \n",
        "\n",
        "Every instance in the dataset will be a fairly short substring of the whole text, and the RNN will be unrolled only over the length of these substrings. This is called **truncated backpropagation through time**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ6j6a6X4v15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_steps = 100\n",
        "window_length = n_steps + 1  # target = input shifted 1 character ahead\n",
        "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61JYVWxK6TTv",
        "colab_type": "text"
      },
      "source": [
        "By default, the window() method creates nonoverlapping windows, but to get the largest possible training set we use shift=1 so that the first window contains characters 0 to 100, the second contains characters 1 to 101, and so on. To ensure that all windows are exactly 101 characters long, we set drop_remainder=True.\n",
        "\n",
        "The window() method creates a dataset that contains windows, each of which is also represented as a dataset. It’s a nested dataset, analogous to a list of lists. This is useful when you want to transform each window by calling its dataset methods (e.g., to shuffle them or batch them).\n",
        "\n",
        "However, we cannot use a nested dataset directly for training, as our model will expect tensors as input, not datasets. So, we must call the flat_map() method: it converts a nested dataset into a flat dataset (one that does not contain datasets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqRLr87U6CnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaSg8Lyd7OI2",
        "colab_type": "text"
      },
      "source": [
        "Notice that we call batch(window_length) on each window: since all windows have exactly that length, we will get a single tensor for each of them. \n",
        "\n",
        "Now the dataset contains consecutive windows of 101 characters each. Since Gradient Descent works best when the instances in the training set are independent and identically distributed.\n",
        "\n",
        "we need to shuffle these windows. Then we can batch the windows and separate the inputs (the first 100 characters) from the target (the last character):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhRdyJc77ndv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "batch_size = 32\n",
        "dataset = dataset.shuffle(10000).batch(batch_size)\n",
        "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8Bf-zAk7-_2",
        "colab_type": "text"
      },
      "source": [
        "<img src='https://github.com/rahiakela/img-repo/blob/master/hands-on-machine-learning-keras-tensorflow/shuffled-windows.png?raw=1' width='800'/>\n",
        "\n",
        "It summarizes the dataset preparation steps discussed so far (showing windows of length 11 rather than 101, and a batch size of 3 instead of 32).\n",
        "\n",
        "Categorical input features should generally be encoded,\n",
        "usually as one-hot vectors or as embeddings. Here, we will encode each character using a one-hot vector because there are fairly few distinct characters(only 39)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHgswkUB9CYg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6f5f92ca-4b87-4601-9787-9b0b7622dc05"
      },
      "source": [
        "dataset = dataset.map(lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
        "dataset.take(1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((None, None, 39), (None, None)), types: (tf.float32, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i401O6iS9_WJ",
        "colab_type": "text"
      },
      "source": [
        "Finally, we just need to add prefetching."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzU8ccaE9-WJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "0885b167-09e7-438f-f881-a010b7f0640d"
      },
      "source": [
        "dataset = dataset.prefetch(1)\n",
        "dataset.take(1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((None, None, 39), (None, None)), types: (tf.float32, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwMFHFDpCLrS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e2eaa2e4-de3c-4cd9-b282-c821889a7b16"
      },
      "source": [
        "for X_batch, Y_batch in dataset.take(1):\n",
        "  print(X_batch.shape, Y_batch.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 100, 39) (32, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptwCwS78-Iga",
        "colab_type": "text"
      },
      "source": [
        "That’s it! Preparing the dataset was the hardest part. Now let’s create the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlSKhb6r-JOI",
        "colab_type": "text"
      },
      "source": [
        "## Building and Training the Char-RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3VfK0wp-LuD",
        "colab_type": "text"
      },
      "source": [
        "To predict the next character based on the previous 100 characters, we can use an RNN with 2 GRU layers of 128 units each and 20% dropout on both the inputs (dropout) and the hidden states (recurrent_dropout). We can tweak these hyperparameters\n",
        "later, if needed. The output layer is a time-distributed Dense layer.\n",
        "\n",
        "This time this layer must have 39 units (max_id) because there are 39 distinct characters in the text, and we want to output a probability for each possible character (at each time step). The output probabilities should sum up to 1 at each time step, so we apply the softmax activation function to the outputs of the Dense layer. \n",
        "\n",
        "We can then compile this model, using the \"sparse_categorical_crossentropy\" loss and an Adam optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5_bM7ns-Ef1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "d18aaa96-feda-4353-ff1a-f3f374a8f939"
      },
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
        "    keras.layers.GRU(128, return_sequences=True),\n",
        "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))                            \n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "gru (GRU)                    (None, None, 128)         64896     \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (None, None, 128)         99072     \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, None, 39)          5031      \n",
            "=================================================================\n",
            "Total params: 168,999\n",
            "Trainable params: 168,999\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NofAusbtDkBI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "bc17fc0a-1bf5-4731-d0e0-cf6df555f51b"
      },
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "history = model.fit(dataset, steps_per_epoch=train_size // batch_size, epochs=10)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 31370 steps\n",
            "Epoch 1/10\n",
            "31370/31370 [==============================] - 479s 15ms/step - loss: 0.9506\n",
            "Epoch 2/10\n",
            "31370/31370 [==============================] - 474s 15ms/step - loss: 0.9554\n",
            "Epoch 3/10\n",
            "31370/31370 [==============================] - 474s 15ms/step - loss: 1.0179\n",
            "Epoch 4/10\n",
            "31370/31370 [==============================] - 481s 15ms/step - loss: 1.0676\n",
            "Epoch 5/10\n",
            "31370/31370 [==============================] - 485s 15ms/step - loss: 1.1008\n",
            "Epoch 6/10\n",
            "31370/31370 [==============================] - 486s 15ms/step - loss: 1.1257\n",
            "Epoch 7/10\n",
            "31370/31370 [==============================] - 482s 15ms/step - loss: 1.1633\n",
            "Epoch 8/10\n",
            "16192/31370 [==============>...............] - ETA: 3:53 - loss: 1.1923Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAzn3WV2Or79",
        "colab_type": "text"
      },
      "source": [
        "## Using the Char-RNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQYwTAGmOtL1",
        "colab_type": "text"
      },
      "source": [
        "Now we have a model that can predict the next character in text written by Shakespeare. To feed it some text, we first need to preprocess it like we did earlier, so let’s create a little function for this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHxJq8dkD0JX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(texts):\n",
        "  X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
        "  return tf.one_hot(X, max_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isDhjrcg8cFY",
        "colab_type": "text"
      },
      "source": [
        "Now let’s use the model to predict the next letter in some text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0i58Gcz8c2m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b6328ff2-8629-4ee3-edb0-3dbcd37ce7b2"
      },
      "source": [
        "X_new = preprocess(['How are yo'])\n",
        "Y_pred = model.predict_classes(X_new)\n",
        "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]  # 1st sentence, last char"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'u'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfH1n6iM9Iqr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "afffb946-7fed-4869-85e5-4af884c92274"
      },
      "source": [
        "tokenizer.sequences_to_texts(Y_pred + 1)[0]  # # 1st sentence, all chars"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'e l   h n e   t o u'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug86x-C889QC",
        "colab_type": "text"
      },
      "source": [
        "Success! The model guessed right. Now let’s use this model to generate new text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gnXqqo68-GA",
        "colab_type": "text"
      },
      "source": [
        "## Generating Fake Shakespearean Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhqjOaXI9gRy",
        "colab_type": "text"
      },
      "source": [
        "To generate new text using the Char-RNN model, we could feed it some text, make the model predict the most likely next letter, add it at the end of the text, then give the extended text to the model to guess the next letter, and so on. But in practice this often leads to the same words being repeated over and over again. \n",
        "\n",
        "Instead, we can pick the next character randomly, with a probability equal to the estimated probability, using TensorFlow’s tf.random.categorical() function. This will generate more diverse and interesting text. The categorical() function samples random class indices, given the class log probabilities (logits). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4diFK4K86F2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8e96d9ce-a15e-4e55-f23a-a17eb926846c"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
              "        2, 0, 0, 1, 1, 1, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i1gQztO_CHO",
        "colab_type": "text"
      },
      "source": [
        "To have more control over the diversity of the generated text, we can divide the logits by a number called the temperature, which we can tweak as we wish: a temperature close to 0 will favor the highprobability characters, while a very high temperature will give all characters an equal probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiSMh9Bw-zuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def next_char(text, temperature=1):\n",
        "  X_new = preprocess([text])\n",
        "  y_proba = model.predict(X_new)[0, -1:, :]\n",
        "  rescaled_logits = tf.math.log(y_proba) / temperature\n",
        "  char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
        "\n",
        "  return tokenizer.sequences_to_texts(char_id.numpy())[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UL8cBT8_rkj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "08c5c6f1-ceb8-4866-bcef-c774f842705b"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "next_char('How are yo', temperature=1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'u'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdhNYGjo_50R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2d9160f4-63f6-47f4-c717-8a56f2b04fd3"
      },
      "source": [
        "next_char('How is lif', temperature=1)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'e'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be5O71gYAPJc",
        "colab_type": "text"
      },
      "source": [
        "We can write a small function that will repeatedly call next_char() to get the next character and append it to the given text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5KMC4tK__L1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def complete_text(text, n_chars=100, temperature=1):\n",
        "  for _ in range(n_chars):\n",
        "    text += next_char(text, temperature)\n",
        "  return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_TuQYOSCVcx",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to generate some text! Let’s try with different temperatures:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKXJ5bUOAuIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "print(complete_text('t', temperature=0.2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QV0xKuEA2UZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "91158985-fbae-48d6-e21d-cf772e8404a5"
      },
      "source": [
        "print(complete_text('t', temperature=1))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tpend me\n",
            "they well makes you what they caruck-hansmity; where her, tread of as her tongue by cannibte\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQBVv41iBggA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e3927509-e044-42b7-a048-39f89e5a2209"
      },
      "source": [
        "print(complete_text('t', temperature=2))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "t as's soltion, weed ins;\n",
            "susizivar iddest three\n",
            "bosojuate;\n",
            "ne's by will fonby thirst's my, music, aw\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDYIjVxuBjR_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "cf537518-2549-4c67-9188-83188478941e"
      },
      "source": [
        "print(complete_text('p', temperature=0.2))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pose and and me this tongue and she do it shall be the citizen:\n",
            "what the other comes the consemvica:\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5Lt3Iw-CvVJ",
        "colab_type": "text"
      },
      "source": [
        "Apparently our Shakespeare model works best at a temperature close to 1. To generate more convincing text, you could try using more GRU layers and more neurons per layer, train for longer, and add some regularization.\n",
        "\n",
        "Moreover, the model is currently incapable of learning patterns longer than n_steps, which is just 100 characters. You could try\n",
        "making this window larger, but it will also make training harder, and even LSTM and GRU cells cannot handle very long sequences. \n",
        "\n",
        "Alternatively, you could use a stateful RNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks5X0LBXDCn8",
        "colab_type": "text"
      },
      "source": [
        "## Stateful RNN"
      ]
    }
  ]
}